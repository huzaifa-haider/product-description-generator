{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchtext transformers rouge-score pandas numpy matplotlib"
      ],
      "metadata": {
        "id": "T57f5LLAajo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6aef72-255e-479f-d239-2f6fde9b9fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3e187dd94b5fd88d8f62b1bafd1e21016cd2ab60993db32953797278e35209b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score, torchtext\n",
            "Successfully installed rouge-score-0.1.2 torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **V1**"
      ],
      "metadata": {
        "id": "GooSoHyvhf0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "class ProductDescriptionDataset(Dataset):\n",
        "    def __init__(self, products, descriptions, tokenizer, max_length=128):\n",
        "        assert len(products) == len(descriptions), \"Products and descriptions must have same length\"\n",
        "        self.products = products\n",
        "        self.descriptions = descriptions\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.products)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            product = str(self.products[idx])\n",
        "            description = str(self.descriptions[idx])\n",
        "\n",
        "            # Tokenize inputs\n",
        "            product_encoding = self.tokenizer(\n",
        "                product,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            description_encoding = self.tokenizer(\n",
        "                description,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'product_ids': product_encoding['input_ids'].squeeze(),\n",
        "                'product_mask': product_encoding['attention_mask'].squeeze(),\n",
        "                'description_ids': description_encoding['input_ids'].squeeze(),\n",
        "                'description_mask': description_encoding['attention_mask'].squeeze()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item at index {idx}: {e}\")\n",
        "            print(f\"Product: {self.products[idx]}\")\n",
        "            print(f\"Description: {self.descriptions[idx]}\")\n",
        "            raise e\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Transformer Model\n",
        "class ProductTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_layer.bias.data.zero_()\n",
        "        self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # Create padding masks\n",
        "        src_padding_mask = (src == 0).to(src.device)  # Assuming 0 is the padding token\n",
        "        tgt_padding_mask = (tgt == 0).to(tgt.device)\n",
        "\n",
        "        # Create causal mask for decoder\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_causal_mask = self.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
        "\n",
        "        # Embed and position encode\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = src.transpose(0, 1)  # Convert to shape [seq_len, batch_size, embed_dim]\n",
        "\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        tgt = tgt.transpose(0, 1)  # Convert to shape [seq_len, batch_size, embed_dim]\n",
        "\n",
        "        # Pass through transformer\n",
        "        memory = self.transformer_encoder(src, src_key_padding_mask=src_padding_mask)\n",
        "        output = self.transformer_decoder(tgt, memory,\n",
        "                                       tgt_mask=tgt_causal_mask,\n",
        "                                       tgt_key_padding_mask=tgt_padding_mask,\n",
        "                                       memory_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        output = output.transpose(0, 1)  # Convert back to [batch_size, seq_len, embed_dim]\n",
        "        return self.output_layer(output)\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            src = batch['product_ids'].to(device)\n",
        "            tgt = batch['description_ids'].to(device)\n",
        "\n",
        "            # Shift target for teacher forcing\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)  # Remove mask arguments\n",
        "\n",
        "            loss = criterion(output.view(-1, output.size(-1)),\n",
        "                           tgt_output.contiguous().view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error in batch: {e}\")\n",
        "            continue\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch['product_ids'].to(device)\n",
        "            tgt = batch['description_ids'].to(device)\n",
        "            tgt_mask = batch['description_mask'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Main training loop\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, device,\n",
        "                num_epochs=10, patience=3):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
        "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print('Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Calculate BLEU and ROUGE scores\n",
        "# def calculate_metrics(model, test_dataloader, tokenizer, device):\n",
        "#     model.eval()\n",
        "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "#     bleu_scores = []\n",
        "#     rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataloader:\n",
        "#             src = batch['product_ids'].to(device)\n",
        "#             tgt = batch['description_ids'].to(device)\n",
        "\n",
        "#             # Generate description\n",
        "#             output = model(src, tgt[:, :-1])\n",
        "#             predicted_ids = torch.argmax(output, dim=-1)\n",
        "\n",
        "#             # Convert ids to text\n",
        "#             predicted_texts = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "#             target_texts = tokenizer.batch_decode(tgt, skip_special_tokens=True)\n",
        "\n",
        "#             # Calculate BLEU\n",
        "#             for pred, target in zip(predicted_texts, target_texts):\n",
        "#                 bleu = bleu_score([pred.split()], [[target.split()]])\n",
        "#                 bleu_scores.append(bleu)\n",
        "\n",
        "#                 # Calculate ROUGE\n",
        "#                 scores = scorer.score(pred, target)\n",
        "#                 for key in rouge_scores:\n",
        "#                     rouge_scores[key].append(scores[key].fmeasure)\n",
        "\n",
        "#     avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "#     avg_rouge = {key: sum(scores) / len(scores) for key, scores in rouge_scores.items()}\n",
        "\n",
        "#     return avg_bleu, avg_rouge\n",
        "\n",
        "# Plot training curves\n",
        "def plot_training_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Hyperparameters\n",
        "hyperparams = {\n",
        "    'd_model': 512,\n",
        "    'nhead': 8,\n",
        "    'num_encoder_layers': 6,\n",
        "    'num_decoder_layers': 6,\n",
        "    'dim_feedforward': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 0.0001,\n",
        "    'num_epochs': 2,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GenAI/Project/clean.csv')\n",
        "\n",
        "# Make sure the required columns exist\n",
        "if 'product' not in df.columns or 'description' not in df.columns:\n",
        "    raise ValueError(\"DataFrame must contain 'product' and 'description' columns\")\n",
        "\n",
        "# Clean the data\n",
        "df = df.dropna(subset=['product', 'description'])  # Remove rows with NaN values\n",
        "df = df.reset_index(drop=True)  # Reset index after dropping NaN values\n",
        "\n",
        "# Split data\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Reset indices for all splits\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# Verify data\n",
        "print(\"Dataset sizes:\")\n",
        "print(f\"Training set: {len(train_df)}\")\n",
        "print(f\"Validation set: {len(val_df)}\")\n",
        "print(f\"Test set: {len(test_df)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProductDescriptionDataset(train_df['product'].values,\n",
        "                                        train_df['description'].values,\n",
        "                                        tokenizer)\n",
        "val_dataset = ProductDescriptionDataset(val_df['product'].values,\n",
        "                                      val_df['description'].values,\n",
        "                                      tokenizer)\n",
        "test_dataset = ProductDescriptionDataset(test_df['product'].values,\n",
        "                                       test_df['description'].values,\n",
        "                                       tokenizer)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                            batch_size=hyperparams['batch_size'],\n",
        "                            shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                           batch_size=hyperparams['batch_size'])\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                           batch_size=hyperparams['batch_size'])\n",
        "\n",
        "# Print sample data\n",
        "print(\"\\nSample data from training set:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ProductTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=hyperparams['d_model'],\n",
        "    nhead=hyperparams['nhead'],\n",
        "    num_encoder_layers=hyperparams['num_encoder_layers'],\n",
        "    num_decoder_layers=hyperparams['num_decoder_layers'],\n",
        "    dim_feedforward=hyperparams['dim_feedforward'],\n",
        "    dropout=hyperparams['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Initialize optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(\n",
        "    model, train_dataloader, val_dataloader, optimizer, criterion, device,\n",
        "    num_epochs=hyperparams['num_epochs'],\n",
        "    patience=hyperparams['patience']\n",
        ")\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_curves(train_losses, val_losses)\n",
        "\n",
        "# # Calculate metrics\n",
        "# avg_bleu, avg_rouge = calculate_metrics(model, test_dataloader, tokenizer, device)\n",
        "# print(f'Average BLEU score: {avg_bleu:.4f}')\n",
        "# print('Average ROUGE scores:')\n",
        "# for key, value in avg_rouge.items():\n",
        "#     print(f'{key}: {value:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "1ux-qygphmrA",
        "outputId": "24bde323-f609-4442-d924-25fc30646b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sizes:\n",
            "Training set: 941\n",
            "Validation set: 202\n",
            "Test set: 202\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2a5a959d1224>\u001b[0m in \u001b[0;36m<cell line: 305>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m train_dataset = ProductDescriptionDataset(train_df['product'].values,\n\u001b[1;32m    306\u001b[0m                                         \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                                         tokenizer)\n\u001b[0m\u001b[1;32m    308\u001b[0m val_dataset = ProductDescriptionDataset(val_df['product'].values,\n\u001b[1;32m    309\u001b[0m                                       \u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3cxBd63so8h",
        "outputId": "14cc2498-48c4-48cb-ada5-6bd16919ce4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "class ProductDescriptionGenerator:\n",
        "    def __init__(self, model_path, tokenizer, device=None):\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = self.load_model(model_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        # Initialize model with the same architecture\n",
        "        model = ProductTransformer(\n",
        "            vocab_size=self.tokenizer.vocab_size,\n",
        "            d_model=512,  # Use the same hyperparameters as during training\n",
        "            nhead=8,\n",
        "            num_encoder_layers=6,\n",
        "            num_decoder_layers=6,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load the saved state dict\n",
        "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        return model\n",
        "\n",
        "    def generate_description(self, product_name, max_length=128, temperature=0.7, top_k=50):\n",
        "        \"\"\"\n",
        "        Generate a description for a given product name.\n",
        "\n",
        "        Args:\n",
        "            product_name (str): Name of the product\n",
        "            max_length (int): Maximum length of generated description\n",
        "            temperature (float): Sampling temperature (higher = more creative, lower = more focused)\n",
        "            top_k (int): Number of top tokens to sample from\n",
        "\n",
        "        Returns:\n",
        "            str: Generated description\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Tokenize input\n",
        "            product_encoding = self.tokenizer(\n",
        "                product_name,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move input to device\n",
        "            src = product_encoding['input_ids'].to(self.device)\n",
        "\n",
        "            # Initialize target with start token\n",
        "            tgt = torch.tensor([[self.tokenizer.cls_token_id]]).to(self.device)\n",
        "\n",
        "            # Generate tokens one by one\n",
        "            for _ in range(max_length):\n",
        "                # Generate prediction\n",
        "                output = self.model(src, tgt)\n",
        "\n",
        "                # Get the next token probabilities\n",
        "                next_token_logits = output[:, -1, :] / temperature\n",
        "\n",
        "                # Apply top-k sampling\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
        "                prob_distribution = torch.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "                # Sample from the distribution\n",
        "                next_token_idx = torch.multinomial(prob_distribution, num_samples=1)\n",
        "                next_token = top_k_indices[:, next_token_idx.squeeze()]\n",
        "\n",
        "                # Stop if we predict the end token\n",
        "                if next_token.item() == self.tokenizer.sep_token_id:\n",
        "                    break\n",
        "\n",
        "                # Concatenate next token to target sequence\n",
        "                tgt = torch.cat([tgt, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # Decode the generated sequence\n",
        "            generated_description = self.tokenizer.decode(tgt.squeeze().tolist(),\n",
        "                                                       skip_special_tokens=True)\n",
        "\n",
        "            return generated_description\n",
        "\n",
        "    def generate_batch_descriptions(self, product_names, batch_size=32, **kwargs):\n",
        "        \"\"\"\n",
        "        Generate descriptions for a list of products in batches.\n",
        "\n",
        "        Args:\n",
        "            product_names (list): List of product names\n",
        "            batch_size (int): Batch size for generation\n",
        "            **kwargs: Additional arguments for generate_description\n",
        "\n",
        "        Returns:\n",
        "            list: List of generated descriptions\n",
        "        \"\"\"\n",
        "        descriptions = []\n",
        "\n",
        "        for i in range(0, len(product_names), batch_size):\n",
        "            batch = product_names[i:i + batch_size]\n",
        "            batch_descriptions = [self.generate_description(name, **kwargs)\n",
        "                                for name in batch]\n",
        "            descriptions.extend(batch_descriptions)\n",
        "\n",
        "        return descriptions\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = ProductDescriptionGenerator(\n",
        "        model_path='best_model.pt',\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Single product generation\n",
        "    product_name = \"Wireless Bluetooth Headphones\"\n",
        "    description = generator.generate_description(\n",
        "        product_name,\n",
        "        max_length=128,\n",
        "        temperature=0.7,\n",
        "        top_k=50\n",
        "    )\n",
        "    print(f\"\\nProduct: {product_name}\")\n",
        "    print(f\"Generated Description: {description}\")\n",
        "\n",
        "    # Batch generation example\n",
        "    product_names = [\n",
        "        \"Smart Watch with Heart Rate Monitor\",\n",
        "        \"Ultra HD 4K TV 55-inch\",\n",
        "        \"Professional Coffee Maker\"\n",
        "    ]\n",
        "\n",
        "    descriptions = generator.generate_batch_descriptions(\n",
        "        product_names,\n",
        "        batch_size=2,\n",
        "        max_length=128,\n",
        "        temperature=0.7,\n",
        "        top_k=50\n",
        "    )\n",
        "\n",
        "    print(\"\\nBatch Generation Results:\")\n",
        "    for product, desc in zip(product_names, descriptions):\n",
        "        print(f\"\\nProduct: {product}\")\n",
        "        print(f\"Generated Description: {desc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CmM6EVD4hm5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03caf09b-fae2-4815-a8cf-82610ea1b2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f624abc7d5f8>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Product: Wireless Bluetooth Headphones\n",
            "Generated Description: unleash yoursh with the with this features for kids, - quality, this fun, and, and easy to your, and alea, these. a must - quality, and a fun!\n",
            "\n",
            "Batch Generation Results:\n",
            "\n",
            "Product: Smart Watch with Heart Rate Monitor\n",
            "Generated Description: unleash yoursh your with this -'s the with the inner a orange! this adorable's a must - quality, this set! this and a, this a touch of all for kids and a must - a must - quality, or a must - the perfect for kids. perfect for kids and # kids, and a must - quality, this # kids # your # kids.\n",
            "\n",
            "Product: Ultra HD 4K TV 55-inch\n",
            "Generated Description: adorable yoursh's with the 3 - for kids! perfect for kids, and a must - quality, and a must - quality, or plush for kids, and. shop now!\n",
            "\n",
            "Product: Professional Coffee Maker\n",
            "Generated Description: unleash yoursh's with the features! this action with this set! this - quality, this fun, a must - a must - quality, and with this adorable and of all, these, this, and a must - quality, and a, and your and a must - quality, and a must - quality, and easy. shop now for kids's. perfect for kids and now for kids and fun, it's more. #, and easy to your. shop now!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}